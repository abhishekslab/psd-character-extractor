# PixiJS Avatar Demo

This is a reference implementation of a speaking avatar runtime using PixiJS. It demonstrates how to load and animate avatar bundles generated by the PSD Character Extractor.

## Features

- **Avatar Loading**: Load avatar.json and atlas.png files
- **Expression States**: Switch between different facial expressions
- **Manual Viseme Control**: Set mouth shapes for lipsync
- **Speech Simulation**: Basic text-to-viseme animation
- **Emotion Parameters**: Valence and arousal sliders
- **Auto Blink**: Automatic blinking animation
- **Real-time Control**: Interactive controls for all parameters

## Usage

1. **Generate Avatar Bundle**: Use the PSD Character Extractor to create an avatar bundle:
   ```bash
   psd-ingest character.psd --output ./avatar_bundle
   ```

2. **Open Demo**: Open `index.html` in a web browser

3. **Load Files**:
   - Click "Choose avatar.json file" and select your avatar.json
   - Click "Choose atlas.png file" and select your atlas.png

4. **Control Avatar**:
   - Use the Expression dropdown to change facial expressions
   - Adjust Valence/Arousal sliders for emotional parameters
   - Enter text and click "Start Speaking" for speech animation
   - Toggle Auto Blink on/off

## Avatar Bundle Format

The demo expects avatar bundles in the following format:

### avatar.json
```json
{
  "meta": {
    "name": "Character Name",
    "source": "character.psd"
  },
  "images": {
    "atlas": "atlas.png",
    "slices": {
      "Face/Eye[L]/state/open": {"x": 0, "y": 0, "w": 50, "h": 30},
      "Face/Mouth/viseme/AI": {"x": 100, "y": 50, "w": 40, "h": 20}
    }
  },
  "slots": {
    "EyeL": {"states": ["open", "closed", "half"]},
    "EyeR": {"states": ["open", "closed", "half"]},
    "Mouth": {"visemes": ["REST", "AI", "E", "U", "O", "FV", "L", "WQ", "MBP"]}
  }
}
```

### atlas.png
PNG image containing all the sprite slices packed together.

## Supported Features

### Expression States
- **IdleNeutral**: Default relaxed state
- **IdleBlink**: Eyes closed for blinking
- **TalkNeutral**: Mouth ready for speech animation
- **Smile**: Happy expression with raised brows
- **Sad**: Sad expression with lowered features
- **Surprised**: Wide eyes and open mouth

### Visemes (Mouth Shapes)
- **REST**: Neutral mouth position
- **AI**: Open vowels (A, I sounds)
- **E**: E vowel sound
- **U**: U vowel sound
- **O**: O vowel sound
- **FV**: F, V consonants
- **L**: L consonant
- **WQ**: W, R consonants
- **MBP**: M, B, P consonants
- **SIL**: Silence/closed mouth

### Slot Types
- **Eyes**: EyeL, EyeR with states (open, closed, half, happy, sad)
- **Mouth**: Visemes and emotions (smile, frown, etc.)
- **Brows**: BrowL, BrowR with shapes (neutral, up, down)

## Technical Details

### PixiJS Integration
- Uses PixiJS v7.3.2 for WebGL-accelerated rendering
- Efficient sprite batching for smooth animation
- Texture atlas loading and UV coordinate mapping

### Animation System
- State-based animation with smooth transitions
- Automatic blink cycle with random intervals
- Speech animation with viseme sequencing
- Emotion parameter modulation

### Performance
- Optimized for 60fps animation
- Minimal memory footprint
- GPU-accelerated rendering via WebGL

## Browser Compatibility

- Chrome 90+
- Firefox 88+
- Safari 14+
- Edge 90+

Requires WebGL support for optimal performance.

## Development

To extend this demo:

1. **Add New States**: Define new expression configurations in the `stateConfigs` object
2. **Improve Speech**: Integrate with Web Speech API or audio analysis
3. **Add Physics**: Implement hair/clothing physics with matter.js
4. **Export Integration**: Add hooks for Live2D, VRM, or other avatar formats

## Limitations

This is a reference implementation focused on demonstrating core functionality:

- Basic positioning system (production would use proper anchor points)
- Simple text-to-viseme conversion (real TTS integration recommended)
- No audio synchronization (would need WebAudio integration)
- Limited expression blending (linear interpolation only)

For production use, consider integrating with:
- **Rhubarb** for accurate lipsync from audio
- **Web Speech API** for real-time TTS
- **WebRTC** for real-time communication
- **Live2D** or **VRM** for advanced avatar systems